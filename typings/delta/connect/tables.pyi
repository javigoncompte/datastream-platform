"""
This type stub file was generated by pyright.
"""

import delta.connect.proto as proto
from typing import Dict, List, Optional, Tuple, Union, overload
from delta.connect._typing import ColumnMapping, ExpressionOrColumn, OptionalColumnMapping, OptionalExpressionOrColumn
from delta.connect.plan import CreateDeltaTable
from delta.tables import IdentityGenerator
from pyspark.sql.connect.dataframe import DataFrame
from pyspark.sql.connect.plan import LogicalPlan
from pyspark.sql.connect.session import SparkSession
from pyspark.sql.types import DataType, StructField, StructType

class DeltaTable:
    __doc__ = ...
    def __init__(self, spark: SparkSession, path: Optional[str] = ..., tableOrViewName: Optional[str] = ..., hadoopConf: Dict[str, str] = ..., plan: Optional[LogicalPlan] = ...) -> None:
        ...
    
    def toDF(self) -> DataFrame:
        ...
    
    def alias(self, aliasName: str) -> DeltaTable:
        ...
    
    def generate(self, mode: str) -> None:
        ...
    
    def delete(self, condition: OptionalExpressionOrColumn = ...) -> DataFrame:
        ...
    
    @overload
    def update(self, condition: ExpressionOrColumn, set: ColumnMapping) -> None:
        ...
    
    @overload
    def update(self, *, set: ColumnMapping) -> None:
        ...
    
    def update(self, condition: OptionalExpressionOrColumn = ..., set: OptionalColumnMapping = ...) -> DataFrame:
        ...
    
    def merge(self, source: DataFrame, condition: ExpressionOrColumn) -> DeltaMergeBuilder:
        ...
    
    def vacuum(self, retentionHours: Optional[float] = ...) -> DataFrame:
        ...
    
    def history(self, limit: Optional[int] = ...) -> DataFrame:
        ...
    
    def detail(self) -> DataFrame:
        ...
    
    @classmethod
    def convertToDelta(cls, sparkSession: SparkSession, identifier: str, partitionSchema: Optional[Union[str, StructType]] = ...) -> DeltaTable:
        ...
    
    @classmethod
    def forPath(cls, sparkSession: SparkSession, path: str, hadoopConf: Dict[str, str] = ...) -> DeltaTable:
        ...
    
    @classmethod
    def forName(cls, sparkSession: SparkSession, tableOrViewName: str) -> DeltaTable:
        ...
    
    @classmethod
    def create(cls, sparkSession: Optional[SparkSession] = ...) -> DeltaTableBuilder:
        ...
    
    @classmethod
    def createIfNotExists(cls, sparkSession: Optional[SparkSession] = ...) -> DeltaTableBuilder:
        ...
    
    @classmethod
    def replace(cls, sparkSession: Optional[SparkSession] = ...) -> DeltaTableBuilder:
        ...
    
    @classmethod
    def createOrReplace(cls, sparkSession: Optional[SparkSession] = ...) -> DeltaTableBuilder:
        ...
    
    @classmethod
    def isDeltaTable(cls, sparkSession: SparkSession, identifier: str) -> bool:
        ...
    
    def upgradeTableProtocol(self, readerVersion: int, writerVersion: int) -> None:
        ...
    
    def addFeatureSupport(self, featureName: str) -> None:
        ...
    
    def restoreToVersion(self, version: int) -> DataFrame:
        ...
    
    def restoreToTimestamp(self, timestamp: str) -> DataFrame:
        ...
    
    def optimize(self) -> DeltaOptimizeBuilder:
        ...
    
    def clone(self, target: str, isShallow: bool = ..., replace: bool = ..., properties: Optional[Dict[str, str]] = ...) -> DeltaTable:
        ...
    
    def cloneAtVersion(self, version: int, target: str, isShallow: bool = ..., replace: bool = ..., properties: Optional[Dict[str, str]] = ...) -> DeltaTable:
        ...
    
    def cloneAtTimestamp(self, timestamp: str, target: str, isShallow: bool = ..., replace: bool = ..., properties: Optional[Dict[str, str]] = ...) -> DeltaTable:
        ...
    


class DeltaMergeBuilder:
    __doc__ = ...
    def __init__(self, spark: SparkSession, target: LogicalPlan, source: LogicalPlan, condition: ExpressionOrColumn) -> None:
        ...
    
    @overload
    def whenMatchedUpdate(self, condition: OptionalExpressionOrColumn, set: ColumnMapping) -> DeltaMergeBuilder:
        ...
    
    @overload
    def whenMatchedUpdate(self, *, set: ColumnMapping) -> DeltaMergeBuilder:
        ...
    
    def whenMatchedUpdate(self, condition: OptionalExpressionOrColumn = ..., set: OptionalColumnMapping = ...) -> DeltaMergeBuilder:
        ...
    
    def whenMatchedUpdateAll(self, condition: OptionalExpressionOrColumn = ...) -> DeltaMergeBuilder:
        ...
    
    def whenMatchedDelete(self, condition: OptionalExpressionOrColumn = ...) -> DeltaMergeBuilder:
        ...
    
    @overload
    def whenNotMatchedInsert(self, condition: ExpressionOrColumn, values: ColumnMapping) -> DeltaMergeBuilder:
        ...
    
    @overload
    def whenNotMatchedInsert(self, *, values: ColumnMapping = ...) -> DeltaMergeBuilder:
        ...
    
    def whenNotMatchedInsert(self, condition: OptionalExpressionOrColumn = ..., values: OptionalColumnMapping = ...) -> DeltaMergeBuilder:
        ...
    
    def whenNotMatchedInsertAll(self, condition: OptionalExpressionOrColumn = ...) -> DeltaMergeBuilder:
        ...
    
    @overload
    def whenNotMatchedBySourceUpdate(self, condition: OptionalExpressionOrColumn, set: ColumnMapping) -> DeltaMergeBuilder:
        ...
    
    @overload
    def whenNotMatchedBySourceUpdate(self, *, set: ColumnMapping) -> DeltaMergeBuilder:
        ...
    
    def whenNotMatchedBySourceUpdate(self, condition: OptionalExpressionOrColumn = ..., set: OptionalColumnMapping = ...) -> DeltaMergeBuilder:
        ...
    
    def whenNotMatchedBySourceDelete(self, condition: OptionalExpressionOrColumn = ...) -> DeltaMergeBuilder:
        ...
    
    def withSchemaEvolution(self) -> DeltaMergeBuilder:
        ...
    
    def execute(self) -> DataFrame:
        ...
    


class DeltaTableBuilder:
    __doc__ = ...
    def __init__(self, spark: SparkSession, mode: proto.CreateDeltaTable.Mode) -> None:
        ...
    
    def tableName(self, identifier: str) -> DeltaTableBuilder:
        ...
    
    def location(self, location: str) -> DeltaTableBuilder:
        ...
    
    def comment(self, comment: str) -> DeltaTableBuilder:
        ...
    
    def addColumn(self, colName: str, dataType: Union[str, DataType], nullable: bool = ..., generatedAlwaysAs: Optional[Union[str, IdentityGenerator]] = ..., generatedByDefaultAs: Optional[IdentityGenerator] = ..., comment: Optional[str] = ...) -> DeltaTableBuilder:
        ...
    
    def addColumns(self, cols: Union[StructType, List[StructField]]) -> DeltaTableBuilder:
        ...
    
    @overload
    def partitionedBy(self, *cols: str) -> DeltaTableBuilder:
        ...
    
    @overload
    def partitionedBy(self, __cols: Union[List[str], Tuple[str, ...]]) -> DeltaTableBuilder:
        ...
    
    def partitionedBy(self, *cols: Union[str, List[str], Tuple[str, ...]]) -> DeltaTableBuilder:
        ...
    
    @overload
    def clusterBy(self, *cols: str) -> DeltaTableBuilder:
        ...
    
    @overload
    def clusterBy(self, __cols: Union[List[str], Tuple[str, ...]]) -> DeltaTableBuilder:
        ...
    
    def clusterBy(self, *cols: Union[str, List[str], Tuple[str, ...]]) -> DeltaTableBuilder:
        ...
    
    def property(self, key: str, value: str) -> DeltaTableBuilder:
        ...
    
    def execute(self) -> DeltaTable:
        ...
    


class DeltaOptimizeBuilder:
    __doc__ = ...
    def __init__(self, spark: SparkSession, table: DeltaTable) -> None:
        ...
    
    def where(self, partitionFilter: str) -> DeltaOptimizeBuilder:
        ...
    
    def executeCompaction(self) -> DataFrame:
        ...
    
    def executeZOrderBy(self, *cols: Union[str, List[str], Tuple[str, ...]]) -> DataFrame:
        ...
    


