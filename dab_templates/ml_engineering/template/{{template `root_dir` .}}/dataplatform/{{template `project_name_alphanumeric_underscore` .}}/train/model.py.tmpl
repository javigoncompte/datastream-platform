"""Model training for {{template `project_name_alphanumeric_underscore` .}}."""

# COMMAND ----------
from typing import Any, Dict
import mlflow
import pandas as pd
import mlflow.data
import altair as alt
import numpy as np
import polars as pl
from mlflow.models.signature import infer_signature
from dataplatform.core.logger import get_logger
from dataplatform.core.mlflow_experiment import setup_mlflow_experiment

logger = get_logger(__name__)
{{template `project_name_alphanumeric_underscore` .}}_mlflow = setup_mlflow_experiment("{{template `project_name_alphanumeric_underscore` .}}")

# COMMAND ----------
# ==============================================================================
# CONFIGURATION
# ==============================================================================
# All tunable parameters should be defined here.
# Example:
# PARAMS = {"C": 1.0, "penalty": "l1", "solver": "liblinear"}
# ==============================================================================
# DATA PREPARATION
# ==============================================================================

# COMMAND ----------

def create_train_dataset() -> pd.DataFrame:
    """Load and prepare the training dataset."""
    logger.info("Creating training dataset")

    pass


# ==============================================================================
# HELPER FUNCTIONS
# ==============================================================================

# COMMAND ----------

class Model(mlflow.pyfunc.PythonModel):
    def __init__(self, model, transformer):
        self.fitted_model = model
        self.transformer = transformer

    def predict(self, model_input, params=None):
        # Do your preprocessing here for data formats if needed
        model_input = pl.from_pandas(model_input)
        model_input = self.transformer.transform(model_input)
        feature_names = self.transformer.get_feature_names_out()
        model_input = pl.DataFrame(
            model_input, schema={name: pl.Float32 for name in feature_names}
        )
        model_input = model_input.to_pandas(use_pyarrow_extension_array=True)
        if params:
            return self.fitted_model.predict(model_input, **params)
        else:
            return self.fitted_model.predict(model_input)




# ==============================================================================
# PLOTTING
# ==============================================================================

# COMMAND ----------


def plot_predictions_example(preds: pd.DataFrame) -> "alt.Chart":
    """Example function to plot model predictions."""
    import altair as alt

    chart = alt.Chart(preds).mark_bar().encode(x="category", y="value").properties(title="Example Predictions")
    return chart


def save_and_log_chart(chart: "alt.Chart", filename: str) -> None:
    """Save an Altair chart to a file and log it to MLflow."""
    chart.save(filename, format="png", scale_factor=2)
    mlflow.log_image(filename, f"{filename}.png")


def log_plots_to_mlflow(charts: Dict[str, "alt.Chart"]) -> None:
    """Log a dictionary of Altair charts to MLflow."""
    logger.info("Logging plots to MLflow")
    for filename, chart in charts.items():
        save_and_log_chart(chart, filename)


# ==============================================================================
# Preprocessing DATA
# ==============================================================================

# COMMAND ----------


def preprocessing_data(df: pd.DataFrame) -> Dict[str, Any]:
    """Process features for the model."""
    logger.info("Running feature engineering")
    # TODO: Implement your feature engineering logic here.
    # Example:
    # X = df.drop("target", axis=1)
    # y = df["target"]
    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    # return {"X_train": X_train, "X_test": X_test, "y_train": y_train, "y_test": y_test}
    pass


# ==============================================================================
# MODEL TRAINING
# ==============================================================================

# COMMAND ----------

# Model will be saved to mlflow under {{template `project_name_alphanumeric_underscore` .}}
# in Experiments Tab in Databricks
def train_model() -> Tuple[Model, pd.DataFrame, pd.DataFrame]:
    """Train the survival model."""
    logger.info("Starting model training")

    df = create_train_dataset()
    logger.info(f"Successfully loaded training data with {len(df.columns)} columns.")

    train_data = preprocessing_data(df)
    x_train = train_data["x_train"]
    x_test = train_data["x_test"]
    y_train = train_data["y_train"]
    y_test = train_data["y_test"]
    transformer = train_data["transformer"]
    logger.info(
        f"Feature engineering complete. Train shape: {x_train.shape}, \
        Test shape: {x_test.shape} \
        Train columns: {x_train.columns}, \
        Test columns: {x_test.columns}"
    )

    base_model = XGBSEKaplanTree(PARAMS)

    xgbse_model = XGBSEBootstrapEstimator(base_model, n_estimators=N_ESTIMATORS)

    logger.info("Fitting XGBSEBootstrapEstimator model...")
    xgbse_model.fit(
        x_train,
        y_train,
        time_bins=TIME_BINS,
        ci_width=CONFIDENCE_INTERVAL,  # confidence interval
    )
    survival_model = Model(xgbse_model, transformer)
    logger.info("Model fitting complete.")
    return survival_model, y_test, x_test


# COMMAND ----------
# ==============================================================================
# MODEL Logging and Evaluation
# ==============================================================================



@survival_model_experiment
def log_evaluate_model(
    model: Model,
    y_test: pd.DataFrame,
    x_test: pd.DataFrame,
) -> Tuple[str, dict[str, Any]]:
    logger.info("Generated predictions for test sets.")
    preds_test, upper_ci_test, lower_ci_test = model.predict(
        model_input=x_test, params={"return_ci": True}
    )
    mlflow.log_params(PARAMS)
    logger.info("Logging model and metrics to MLflow...")
    signature = infer_signature(x_test, preds_test)
    model_info = mlflow.pyfunc.log_model(
        python_model=model,
        name="{{template `project_name_alphanumeric_underscore` .}}",
        signature=signature,
        extra_pip_requirements=["code/{{template `project_name_alphanumeric_underscore` .}}-0.0.0-py3-none-any.whl"],
        code_paths=[
            "../../../../artifacts/.internal/{{template `project_name_alphanumeric_underscore` .}}-0.0.0-py3-none-any.whl"
        ],
    )

    # Create visualization plots
    chart1 = plot_survival_probability(preds_test, upper_ci_test, lower_ci_test)
    interval_probs = survival_model.predict(
        model_input=x_test, params={"return_interval_probs": True}
    )
    chart2 = plot_interval_probabilities(interval_probs)
    chart3 = plot_mean_survival_probability(preds_test)

    # Log plots to MLflow
    charts_to_log = {
        "./survival_probability_plot.png": chart1,
        "./interval_probabilities.png": chart2,
        "./mean_survival_probability.png": chart3,
    }
    logger.info("Logging plots to MLflow...")
    log_plots_to_mlflow(charts_to_log)

    logger.info("Calculating metrics...")
    concordance_index_test = concordance_index(y_test, preds_test)
    approx_brier_score_test = approx_brier_score(y_test, preds_test, aggregate="mean")
    dist_calibration_score_test = dist_calibration_score(
        y_test, preds_test, returns="pval"
    )

    logger.info(
        f"Metrics calculated - C-index: {concordance_index_test}, "
        f"Brier: {approx_brier_score_test}, D-cal: {dist_calibration_score_test}"
    )

    logger.info("Logging confidence interval datasets to MLflow...")

    lower_ci_dataset = mlflow.data.from_pandas(
        lower_ci_test,
        source="xgbse_bootstrap_estimator",
        name="lower_confidence_intervals",
    )

    upper_ci_dataset = mlflow.data.from_pandas(
        upper_ci_test,
        source="xgbse_bootstrap_estimator",
        name="upper_confidence_intervals",
    )

    # Create dataset for time bins
    time_bins_df = pd.DataFrame({"time_bins": TIME_BINS})
    time_bins_dataset = mlflow.data.from_pandas(
        time_bins_df,
        source="model_configuration",
        name="time_bins",
    )

    mlflow.log_input(lower_ci_dataset, context="confidence_intervals")
    mlflow.log_input(upper_ci_dataset, context="confidence_intervals")
    mlflow.log_input(time_bins_dataset, context="model_configuration")

    metrics = {
        "n_estimators": N_ESTIMATORS,
        "confidence_interval": CONFIDENCE_INTERVAL,
        "c-index-test": concordance_index_test,
        "brier_score-test": approx_brier_score_test,
        "d_calibration-test": dist_calibration_score_test,
    }

    logger.info(f"Final metrics to log: {metrics}")
    mlflow.log_metrics(metrics, model_id=model_info.model_id)
    return model_info.model_uri, metrics


# COMMAND ----------


def run_model_training():
    model, y_test, x_test = train_model()
    model_uri, metrics = log_evaluate_model(model, y_test, x_test)
    logger.info(f"Model URI: {model_uri}")
    logger.info(f"Metrics: {metrics}")
    return model_uri, metrics

